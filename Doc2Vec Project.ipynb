{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553e2c0a-b2e8-413f-9ee4-accabaf2e5a4",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8d63cf-91aa-4067-97cb-c5523b78c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a99623-00af-4e33-8414-0c3a0eda05c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/alex/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d3b6d0-0d08-4897-90e6-8062cbeeeb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df325a00-bfaa-4c0a-8e1c-25933107d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocessing(j):\n",
    "    result = re.sub(r'\\d+', '', j)\n",
    "    result = re.sub(r'[^\\w\\s]', '', result)\n",
    "    result = j.strip()\n",
    "    tokens = word_tokenize(j)\n",
    "    result = [lemmatizer.lemmatize(i) for i in tokens if not i in stop_words and len(i) > 1 and not i.isdigit()]\n",
    "    # is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # nouns = [word for (word, pos) in nltk.pos_tag(result) if is_noun(pos)] \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b47bfda6-a751-44ef-b439-e60335b1fc88",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(st):\n",
    "    st = st.lower()\n",
    "    st = st.strip()\n",
    "    text_tokens = word_tokenize(st)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    filtered_sentence = (\" \").join(tokens_without_sw)\n",
    "    input_st = []\n",
    "    for word in filtered_sentence:\n",
    "        input_st.append(lemmatizer.lemmatize(word))\n",
    "    sentence = (\"\").join(input_st)   \n",
    "    blob = TextBlob(sentence)\n",
    "    noun_phrases = blob.noun_phrases\n",
    "    phrases = []\n",
    "    for phrase in noun_phrases:\n",
    "        phrases.append(phrase)\n",
    "    return phrases\n",
    "    # возвращает список всех nous phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7456730-43aa-4324-b292-de3ecaa3873a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>unique_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>australia’s energy wars leave coal still stand...</td>\n",
       "      <td>environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>buying unique newborn baby gifts\\n\\nnot a long...</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>big bets on long shots\\n\\nexpenditures by the ...</td>\n",
       "      <td>sport and health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>photo by charles deluvio on unsplash\\n\\ntools ...</td>\n",
       "      <td>sport and health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the greatest sin in marketing agencies\\n\\nlets...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  \\\n",
       "0           0  australia’s energy wars leave coal still stand...   \n",
       "1           1  buying unique newborn baby gifts\\n\\nnot a long...   \n",
       "2           2  big bets on long shots\\n\\nexpenditures by the ...   \n",
       "3           3  photo by charles deluvio on unsplash\\n\\ntools ...   \n",
       "4           4  the greatest sin in marketing agencies\\n\\nlets...   \n",
       "\n",
       "          unique_tag  \n",
       "0        environment  \n",
       "1               life  \n",
       "2   sport and health  \n",
       "3   sport and health  \n",
       "4               work  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_d2v = pd.read_csv('/Users/alex/Desktop/ВШЭ/ML/Проекты/Project ML/df for proj.csv', sep=';')\n",
    "df_d2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57891a0a-a51b-4670-9167-9eb538d94917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1033/1033 [00:06<00:00, 159.95it/s]\n",
      "100%|██████████| 443/443 [00:02<00:00, 177.37it/s]\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df_d2v, test_size=0.3, random_state=42)\n",
    "\n",
    "train_tagged = train.progress_apply(\n",
    "    lambda r: TaggedDocument(words=preprocessing(r['text']), tags=[r.unique_tag]), axis=1)\n",
    "test_tagged = test.progress_apply(\n",
    "    lambda r: TaggedDocument(words=preprocessing(r['text']), tags=[r.unique_tag]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f0e38b2-ba55-4277-a40e-5d23b0517913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5aa0b6b-3111-4d3b-a8d5-c8ea7a9fd1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1033/1033 [00:00<00:00, 1497137.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=400, negative=5, hs=0, min_count=1, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff657456-25bd-43ba-9f61-6349b148f97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1033/1033 [00:00<00:00, 3430495.67it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3871953.56it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3844468.53it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3687417.90it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3899834.41it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3601592.71it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3971325.42it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3841060.31it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3858162.09it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3827487.66it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3807307.59it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3854729.57it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3868496.46it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3571901.10it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3924561.62it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3985939.31it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3913925.95it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3841060.31it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3882362.04it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3928119.70it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3784031.47it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3560161.08it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3824109.47it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3946007.32it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3433213.97it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3882362.04it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3928119.70it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3780729.52it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3780729.52it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3703176.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.5 s, sys: 968 ms, total: 34.5 s\n",
      "Wall time: 7.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72422555-0c79-4ad6-8f37-f7843d030f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd1ddd71-bcab-48d3-90bb-7beef48cc0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0823b97-6c83-4eac-93f2-c13a13dbd469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3115e1e1-f041-48c3-998c-6ee7addbe68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.16930022573363432\n",
      "Testing F1 score: 0.16613384770083225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9afa86ca-9eb8-44ef-9a70-45d8e2f86754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1033/1033 [00:00<00:00, 1401719.84it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=400, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7124d584-031f-4a4d-84fc-669c4f058da4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1033/1033 [00:00<00:00, 3353495.38it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3854729.57it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3868496.46it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3854729.57it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3871953.56it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3841060.31it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3810656.14it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3858162.09it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3868496.46it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3800628.10it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3703176.10it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3854729.57it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3868496.46it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3797297.14it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3810656.14it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3971325.42it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3784031.47it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3899834.41it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3931684.24it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3837658.13it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3899834.41it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3824109.47it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3942416.77it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3871953.56it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3910393.53it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3931684.24it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3956818.29it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3971325.42it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3960435.13it/s]\n",
      "100%|██████████| 1033/1033 [00:00<00:00, 3953208.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 597 ms, total: 1min 4s\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6473441e-1dc1-441f-8440-1c04b73532a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dmm, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a9b2a14b-c75c-429d-8477-e278cccb7837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.47404063205417607\n",
      "Testing F1 score: 0.46396115052063774\n"
     ]
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f5e9aa4-140f-4ea9-9e67-ad56bf1d8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef31b674-447f-4a6f-a3fc-7a87fce2218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b04e4b28-fd4d-435f-ae83-97ce2061ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = get_vectors(new_model, train_tagged)\n",
    "y_test, X_test = get_vectors(new_model, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a587f51-d4d9-4960-b4f3-dde9d7bdf48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.4672686230248307\n",
      "Testing F1 score: 0.4563127641705693\n"
     ]
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdddc1-1c83-4b4f-a471-91d8efd09409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
